{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mortality rate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# 1) Load data\n",
    "# Note:\n",
    "# - Both mimicdeathtrain and mimicdeathtest are assumed to be standardized and one-hot encoded already.\n",
    "# - For other prediction tasks, switch files to:\n",
    "#   mimicmodstrain.csv (MODS), mimichxtrain.csv (hypoxemia), mimichstrain.csv (hemorrhagic shock).\n",
    "df_train = pd.read_csv('mimicmodstrain.csv').drop(['stay_id', 'hr'], axis=1)\n",
    "X_train = df_train.iloc[:, :-1].values\n",
    "y_train = df_train.iloc[:, -1].values\n",
    "\n",
    "df_test = pd.read_csv('mimicmodstest.csv')\n",
    "X_test = df_test.iloc[:, :-1].values\n",
    "y_test = df_test.iloc[:, -1].values\n",
    "\n",
    "# 2) Standardization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# --- LightGBM (Bayesian Optimization) ---\n",
    "def lgb_cv(n_estimators, max_depth, learning_rate, subsample, colsample_bytree):\n",
    "    params = {\n",
    "        'n_estimators': int(n_estimators),\n",
    "        'max_depth': int(max_depth),\n",
    "        'learning_rate': learning_rate,\n",
    "        'subsample': subsample,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'objective': 'binary',\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = LGBMClassifier(**params)\n",
    "    return cross_val_score(model, X_train, y_train, scoring='roc_auc', cv=cv).mean()\n",
    "\n",
    "lgb_bo = BayesianOptimization(\n",
    "    f=lgb_cv,\n",
    "    pbounds={\n",
    "        'n_estimators': (100, 500),\n",
    "        'max_depth': (3, 10),\n",
    "        'learning_rate': (0.01, 0.3),\n",
    "        'subsample': (0.5, 1.0),\n",
    "        'colsample_bytree': (0.5, 1.0)\n",
    "    },\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "lgb_bo.maximize(init_points=5, n_iter=15)\n",
    "\n",
    "lgb_best_params = lgb_bo.max['params']\n",
    "lgb_best_params['n_estimators'] = int(lgb_best_params['n_estimators'])\n",
    "lgb_best_params['max_depth'] = int(lgb_best_params['max_depth'])\n",
    "lgbm_best = LGBMClassifier(objective='binary', random_state=42, **lgb_best_params)\n",
    "\n",
    "# --- Logistic Regression (Bayesian Optimization) ---\n",
    "def lr_cv(C, solver_idx):\n",
    "    # Map solver index to solver name\n",
    "    solver_list = ['liblinear', 'lbfgs']\n",
    "    solver = solver_list[int(round(solver_idx))]\n",
    "    model = LogisticRegression(C=C, solver=solver, max_iter=1000, random_state=42)\n",
    "    return cross_val_score(model, X_train, y_train, scoring='roc_auc', cv=cv).mean()\n",
    "\n",
    "lr_bo = BayesianOptimization(\n",
    "    f=lr_cv,\n",
    "    pbounds={\n",
    "        'C': (0.001, 10.0),\n",
    "        'solver_idx': (0, 1)\n",
    "    },\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "lr_bo.maximize(init_points=3, n_iter=7)\n",
    "\n",
    "lr_best_params = lr_bo.max['params']\n",
    "lr_best_solver = ['liblinear', 'lbfgs'][int(round(lr_best_params['solver_idx']))]\n",
    "lr_best_C = lr_best_params['C']\n",
    "lr_best = LogisticRegression(C=lr_best_C, solver=lr_best_solver, max_iter=1000, random_state=42)\n",
    "\n",
    "# --- Random Forest (Bayesian Optimization) ---\n",
    "def rf_cv(n_estimators, max_depth, max_features, min_samples_split):\n",
    "    params = {\n",
    "        'n_estimators': int(n_estimators),\n",
    "        'max_depth': int(max_depth),\n",
    "        'max_features': max_features,\n",
    "        'min_samples_split': int(min_samples_split),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    model = RandomForestClassifier(**params)\n",
    "    return cross_val_score(model, X_train, y_train, scoring='roc_auc', cv=cv).mean()\n",
    "\n",
    "rf_bo = BayesianOptimization(\n",
    "    f=rf_cv,\n",
    "    pbounds={\n",
    "        'n_estimators': (100, 500),\n",
    "        'max_depth': (3, 20),\n",
    "        'max_features': (0.1, 1.0),\n",
    "        'min_samples_split': (2, 20)\n",
    "    },\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "rf_bo.maximize(init_points=5, n_iter=15)\n",
    "\n",
    "rf_best_params = rf_bo.max['params']\n",
    "rf_best_params['n_estimators'] = int(rf_best_params['n_estimators'])\n",
    "rf_best_params['max_depth'] = int(rf_best_params['max_depth'])\n",
    "rf_best_params['min_samples_split'] = int(rf_best_params['min_samples_split'])\n",
    "rf_best = RandomForestClassifier(random_state=42, n_jobs=-1, **rf_best_params)\n",
    "\n",
    "# --- XGBoost (Bayesian Optimization) ---\n",
    "def xgb_cv(n_estimators, max_depth, learning_rate, subsample, colsample_bytree, gamma):\n",
    "    params = {\n",
    "        'n_estimators': int(n_estimators),\n",
    "        'max_depth': int(max_depth),\n",
    "        'learning_rate': learning_rate,\n",
    "        'subsample': subsample,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'gamma': gamma,\n",
    "        'use_label_encoder': False,\n",
    "        'eval_metric': 'logloss',\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = XGBClassifier(**params)\n",
    "    return cross_val_score(model, X_train, y_train, scoring='roc_auc', cv=cv).mean()\n",
    "\n",
    "xgb_bo = BayesianOptimization(\n",
    "    f=xgb_cv,\n",
    "    pbounds={\n",
    "        'n_estimators': (100, 500),\n",
    "        'max_depth': (3, 10),\n",
    "        'learning_rate': (0.01, 0.3),\n",
    "        'subsample': (0.5, 1.0),\n",
    "        'colsample_bytree': (0.5, 1.0),\n",
    "        'gamma': (0.0, 5.0)\n",
    "    },\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "xgb_bo.maximize(init_points=5, n_iter=15)\n",
    "\n",
    "xgb_best_params = xgb_bo.max['params']\n",
    "xgb_best_params['n_estimators'] = int(xgb_best_params['n_estimators'])\n",
    "xgb_best_params['max_depth'] = int(xgb_best_params['max_depth'])\n",
    "xgb_best = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, **xgb_best_params)\n",
    "\n",
    "# 3) Model dictionary (only LR, RF, XGBoost, LightGBM)\n",
    "models = {\n",
    "    'Logistic Regression (BayesOpt)': lr_best,\n",
    "    'Random Forest (BayesOpt)': rf_best,\n",
    "    'XGBoost (BayesOpt)': xgb_best,\n",
    "    'LightGBM (BayesOpt)': lgbm_best\n",
    "}\n",
    "\n",
    "# 4) Train and plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    auc_score = roc_auc_score(y_test, y_proba)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('ROC Curve Comparison (Bayesian-Optimized Models)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
