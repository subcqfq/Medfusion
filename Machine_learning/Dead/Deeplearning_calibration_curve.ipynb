{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8045cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ===== (1) Environment-level reproducibility settings: before importing torch =====\n",
    "import os\n",
    "import random\n",
    "\n",
    "SEED = 20240518  # You can change this to any fixed number\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "# Enable determinism for certain cublas ops (requires CUDA >= 10.2; must be set before first CUDA use)\n",
    "# Allowed values \":16:8\" or \":4096:8\" (larger memory). Ignored if unsupported by your environment.\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "\n",
    "# ===== (2) Common imports =====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, classification_report, brier_score_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ===== (3) Unified global seed setup function =====\n",
    "def set_global_seed(seed: int = 20240518):\n",
    "    # Python / NumPy / PyTorch\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # cuDNN / TF32: avoid nondeterminism or numeric drift\n",
    "    try:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Disable TF32 (some GPUs/drivers enable it by default, which may introduce small differences)\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Strictly use deterministic algorithms (will throw if an op isn't supported; use warn_only=True for warnings)\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "# ===== (4) Device selection (doesn't affect determinism, but affects speed) =====\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# ===== (5) Probability calibration class =====\n",
    "class ProbabilityCalibrator:\n",
    "    \"\"\"Probability calibrator supporting Platt Scaling and Isotonic Regression\"\"\"\n",
    "    \n",
    "    def __init__(self, method='platt'):\n",
    "        \"\"\"\n",
    "        Initialize calibrator\n",
    "        Args:\n",
    "            method: 'platt' for Platt Scaling, 'isotonic' for Isotonic Regression\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        if method == 'platt':\n",
    "            self.calibrator = LogisticRegression()\n",
    "        elif method == 'isotonic':\n",
    "            self.calibrator = IsotonicRegression(out_of_bounds='clip')\n",
    "        else:\n",
    "            raise ValueError(\"Method must be 'platt' or 'isotonic'\")\n",
    "        \n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def fit(self, probabilities, true_labels):\n",
    "        \"\"\"\n",
    "        Train the calibrator on a calibration set\n",
    "        Args:\n",
    "            probabilities: model output probabilities (numpy array)\n",
    "            true_labels: true labels (numpy array)\n",
    "        \"\"\"\n",
    "        probabilities = np.array(probabilities).reshape(-1, 1)\n",
    "        true_labels = np.array(true_labels)\n",
    "        \n",
    "        self.calibrator.fit(probabilities, true_labels)\n",
    "        self.is_fitted = True\n",
    "    \n",
    "    def predict_proba(self, probabilities):\n",
    "        \"\"\"\n",
    "        Calibrate probabilities\n",
    "        Args:\n",
    "            probabilities: probabilities to calibrate (numpy array)\n",
    "        Returns:\n",
    "            Calibrated probabilities (numpy array)\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Calibrator must be fitted before prediction\")\n",
    "        \n",
    "        probabilities = np.array(probabilities).reshape(-1, 1)\n",
    "        \n",
    "        if self.method == 'platt':\n",
    "            return self.calibrator.predict_proba(probabilities)[:, 1]\n",
    "        else:  # isotonic\n",
    "            return self.calibrator.predict(probabilities.flatten())\n",
    "\n",
    "\n",
    "def evaluate_calibration(y_true, y_prob, n_bins=10):\n",
    "    \"\"\"\n",
    "    Evaluate calibration quality\n",
    "    Args:\n",
    "        y_true: true labels\n",
    "        y_prob: predicted probabilities\n",
    "        n_bins: number of bins\n",
    "    Returns:\n",
    "        dict: a dictionary containing various calibration metrics\n",
    "    \"\"\"\n",
    "    fraction_positives, mean_predicted = calibration_curve(\n",
    "        y_true, y_prob, n_bins=n_bins\n",
    "    )\n",
    "    \n",
    "    # Compute Expected Calibration Error (ECE)\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    \n",
    "    ece = 0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = (y_prob > bin_lower) & (y_prob <= bin_upper)\n",
    "        prop_in_bin = in_bin.mean()\n",
    "        \n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = y_true[in_bin].mean()\n",
    "            avg_confidence_in_bin = y_prob[in_bin].mean()\n",
    "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "    \n",
    "    # Brier Score\n",
    "    brier_score = brier_score_loss(y_true, y_prob)\n",
    "    \n",
    "    return {\n",
    "        'ece': ece,\n",
    "        'brier_score': brier_score,\n",
    "        'fraction_positives': fraction_positives,\n",
    "        'mean_predicted': mean_predicted\n",
    "    }\n",
    "\n",
    "\n",
    "# ===== (6) Data loading and preprocessing =====\n",
    "def load_and_preprocess_data():\n",
    "    train_df = pd.read_csv('mimicdeadtrain.csv')\n",
    "    test_df  = pd.read_csv('mimicdeadtest.csv')\n",
    "\n",
    "    print(f\"Train set size: {train_df.shape}\")\n",
    "    print(f\"Test set size: {test_df.shape}\")\n",
    "    print(f\"Train set positive rate: {train_df['death'].mean():.4f}\")\n",
    "    print(f\"Test set positive rate: {test_df['death'].mean():.4f}\")\n",
    "\n",
    "    feature_columns = [c for c in train_df.columns if c != 'death']\n",
    "\n",
    "    X_train = train_df[feature_columns].values\n",
    "    y_train = train_df['death'].values.astype(np.float32)\n",
    "    X_test  = test_df[feature_columns].values\n",
    "    y_test  = test_df['death'].values.astype(np.float32)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "    # Three-way split: train -> train (60%) + calibration (20%) + validation (20%)\n",
    "    X_temp, X_val, y_temp, y_val = train_test_split(\n",
    "        X_train_scaled, y_train, test_size=0.2, random_state=SEED, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    X_tr, X_cal, y_tr, y_cal = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.25, random_state=SEED, stratify=y_temp  # 0.25 * 0.8 = 0.2\n",
    "    )\n",
    "\n",
    "    # Convert to tensors\n",
    "    X_tr  = torch.tensor(X_tr,  dtype=torch.float32)\n",
    "    y_tr  = torch.tensor(y_tr,  dtype=torch.float32)\n",
    "    X_cal = torch.tensor(X_cal, dtype=torch.float32)\n",
    "    y_cal = torch.tensor(y_cal, dtype=torch.float32)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "    X_te  = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "    y_te  = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    print(f\"Sizes after split:\")\n",
    "    print(f\"  Train: {X_tr.shape[0]}\")\n",
    "    print(f\"  Calibration: {X_cal.shape[0]}\")\n",
    "    print(f\"  Validation: {X_val.shape[0]}\")\n",
    "    print(f\"  Test: {X_te.shape[0]}\")\n",
    "\n",
    "    return X_tr, y_tr, X_cal, y_cal, X_val, y_val, X_te, y_te, len(feature_columns)\n",
    "\n",
    "\n",
    "# ===== (7) Model definitions =====\n",
    "# 7.1 MLP\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64, 32], dropout=0.3):\n",
    "        super().__init__()\n",
    "        layers, prev = [], input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers += [nn.Linear(prev, h), nn.BatchNorm1d(h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            prev = h\n",
    "        layers += [nn.Linear(prev, 1)]  # no Sigmoid; use BCEWithLogitsLoss\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze(-1)\n",
    "\n",
    "\n",
    "# 7.2 GRU + Attention\n",
    "class GRUAttnModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Treat each numerical feature as a 'token':\n",
    "    token = value_proj(value) + feature_embedding[feature_id]\n",
    "    Feed tokens to a BiGRU, then use trainable attention for weighted pooling, and finally classify.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, emb_dim=64, hidden_dim=128, num_layers=2,\n",
    "                 bidirectional=True, dropout=0.3, attn_hidden=128):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        # Assign a learnable embedding for each feature position\n",
    "        self.feature_emb = nn.Embedding(input_dim, emb_dim)\n",
    "        # Scalar -> vector\n",
    "        self.val_proj = nn.Linear(1, emb_dim)\n",
    "        self.token_ln = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "        # Attention (additive / HAN-style)\n",
    "        self.attn_fc = nn.Linear(hidden_dim * self.num_directions, attn_hidden)\n",
    "        self.attn_tanh = nn.Tanh()\n",
    "        self.attn_vec = nn.Linear(attn_hidden, 1, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * self.num_directions, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 1)  # no Sigmoid; use BCEWithLogitsLoss\n",
    "        )\n",
    "\n",
    "        # Initialization (controlled by global seed)\n",
    "        nn.init.xavier_uniform_(self.val_proj.weight)\n",
    "        nn.init.zeros_(self.val_proj.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len)  Treat each feature as a time step\n",
    "        B, S = x.shape\n",
    "        feat_ids = torch.arange(S, device=x.device).unsqueeze(0).expand(B, S)  # (B, S)\n",
    "\n",
    "        value_tokens = self.val_proj(x.unsqueeze(-1))            # (B, S, emb_dim)\n",
    "        feat_tokens  = self.feature_emb(feat_ids)                # (B, S, emb_dim)\n",
    "        tokens = self.token_ln(value_tokens + feat_tokens)       # (B, S, emb_dim)\n",
    "        tokens = self.dropout(tokens)\n",
    "\n",
    "        H, _ = self.gru(tokens)  # (B, S, hidden*dirs)\n",
    "\n",
    "        # Attention weights\n",
    "        U = self.attn_tanh(self.attn_fc(H))          # (B, S, attn_hidden)\n",
    "        scores = self.attn_vec(U).squeeze(-1)        # (B, S)\n",
    "        alpha = torch.softmax(scores, dim=1)         # (B, S)\n",
    "        context = torch.sum(H * alpha.unsqueeze(-1), dim=1)  # (B, hidden*dirs)\n",
    "\n",
    "        context = self.dropout(context)\n",
    "        logits = self.classifier(context).squeeze(-1)  # (B,)\n",
    "        return logits  # logits (before sigmoid)\n",
    "\n",
    "\n",
    "# 7.3 Transformer\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=64, nhead=8, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.input_projection = nn.Linear(1, d_model)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(input_dim, d_model))  # controlled by global seed\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
    "                                               dropout=dropout, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, 32), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(32, 1)  # no Sigmoid\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, S = x.size(0), x.size(1)\n",
    "        x = self.input_projection(x.unsqueeze(-1))              # (B, S, d_model)\n",
    "        x = x + self.pos_encoding.unsqueeze(0).expand(B, -1, -1)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.classifier(x).squeeze(-1)\n",
    "\n",
    "\n",
    "# ===== (8) Training and evaluation =====\n",
    "def get_model_predictions(model, data_loader):\n",
    "    \"\"\"Get model predicted probabilities\"\"\"\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in data_loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds.extend(probs.cpu().numpy())\n",
    "            labels.extend(yb.numpy())\n",
    "    return np.array(preds), np.array(labels)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=100, lr=1e-3, pos_weight=None):\n",
    "    if pos_weight is not None:\n",
    "        pos_weight = torch.tensor(pos_weight, dtype=torch.float32, device=device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=10, verbose=False, min_lr=1e-6\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "    best_auc, patience_counter, patience = 0.0, 0, 30\n",
    "    train_losses, val_aucs = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "            running += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        val_preds, val_labels = get_model_predictions(model, val_loader)\n",
    "        val_auc = roc_auc_score(val_labels, val_preds)\n",
    "        scheduler.step(val_auc)\n",
    "\n",
    "        train_losses.append(running / max(1, len(train_loader)))\n",
    "        val_aucs.append(val_auc)\n",
    "\n",
    "        improved = val_auc > best_auc + 1e-6\n",
    "        if improved:\n",
    "            best_auc = val_auc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1:03d}/{epochs} | Train Loss: {train_losses[-1]:.4f} | Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "    return model, best_auc, train_losses, val_aucs\n",
    "\n",
    "\n",
    "def evaluate_model_with_calibration(model, cal_loader, test_loader, model_name):\n",
    "    \"\"\"Evaluate the model and perform probability calibration\"\"\"\n",
    "    # Get predictions on the calibration set\n",
    "    cal_preds, cal_labels = get_model_predictions(model, cal_loader)\n",
    "    \n",
    "    # Get raw predictions on the test set\n",
    "    test_preds, test_labels = get_model_predictions(model, test_loader)\n",
    "    \n",
    "    # Train calibrators\n",
    "    platt_calibrator = ProbabilityCalibrator(method='platt')\n",
    "    isotonic_calibrator = ProbabilityCalibrator(method='isotonic')\n",
    "    \n",
    "    platt_calibrator.fit(cal_preds, cal_labels)\n",
    "    isotonic_calibrator.fit(cal_preds, cal_labels)\n",
    "    \n",
    "    # Calibrate test-set predictions\n",
    "    test_preds_platt = platt_calibrator.predict_proba(test_preds)\n",
    "    test_preds_isotonic = isotonic_calibrator.predict_proba(test_preds)\n",
    "    \n",
    "    # Compute AUC (calibration should not change AUC)\n",
    "    auc_original = roc_auc_score(test_labels, test_preds)\n",
    "    auc_platt = roc_auc_score(test_labels, test_preds_platt)\n",
    "    auc_isotonic = roc_auc_score(test_labels, test_preds_isotonic)\n",
    "    \n",
    "    # Evaluate calibration quality\n",
    "    cal_metrics_original = evaluate_calibration(test_labels, test_preds)\n",
    "    cal_metrics_platt = evaluate_calibration(test_labels, test_preds_platt)\n",
    "    cal_metrics_isotonic = evaluate_calibration(test_labels, test_preds_isotonic)\n",
    "    \n",
    "    # Save prediction results\n",
    "    results_df = pd.DataFrame({\n",
    "        'y_true': test_labels,\n",
    "        'y_proba_original': test_preds,\n",
    "        'y_proba_platt': test_preds_platt,\n",
    "        'y_proba_isotonic': test_preds_isotonic\n",
    "    })\n",
    "    results_df.to_csv(f\"death_{model_name}_calibrated_proba.csv\", index=False)\n",
    "    \n",
    "    return {\n",
    "        'auc_original': auc_original,\n",
    "        'auc_platt': auc_platt,\n",
    "        'auc_isotonic': auc_isotonic,\n",
    "        'calibration_original': cal_metrics_original,\n",
    "        'calibration_platt': cal_metrics_platt,\n",
    "        'calibration_isotonic': cal_metrics_isotonic,\n",
    "        'predictions_original': test_preds,\n",
    "        'predictions_platt': test_preds_platt,\n",
    "        'predictions_isotonic': test_preds_isotonic,\n",
    "        'true_labels': test_labels\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_calibration_curve(y_true, y_prob_dict, model_name):\n",
    "    \"\"\"Plot calibration curve\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    colors = ['blue', 'red', 'green']\n",
    "    methods = list(y_prob_dict.keys())\n",
    "    \n",
    "    for i, (method, y_prob) in enumerate(y_prob_dict.items()):\n",
    "        fraction_positives, mean_predicted = calibration_curve(\n",
    "            y_true, y_prob, n_bins=10\n",
    "        )\n",
    "        plt.plot(mean_predicted, fraction_positives, \n",
    "                marker='o', color=colors[i], label=method, linewidth=2)\n",
    "    \n",
    "    # Perfect calibration line\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect Calibration')\n",
    "    \n",
    "    plt.xlabel('Mean Predicted Probability')\n",
    "    plt.ylabel('Fraction of Positives')\n",
    "    plt.title(f'Calibration Curve - {model_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_name}_calibration_curve.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ===== (9) Main pipeline =====\n",
    "def main():\n",
    "    # Fix random seed before any stochastic operation\n",
    "    set_global_seed(SEED)\n",
    "\n",
    "    X_tr, y_tr, X_cal, y_cal, X_val, y_val, X_te, y_te, input_dim = load_and_preprocess_data()\n",
    "\n",
    "    # Compute positive-class weight\n",
    "    pos_weight = (y_tr == 0).sum().item() / max(1, (y_tr == 1).sum().item())\n",
    "    print(f\"Positive-class weight pos_weight = {pos_weight:.2f}\")\n",
    "\n",
    "    # Sampler: make each batch more balanced (reproducible: fixed generator)\n",
    "    y_tr_np = y_tr.numpy().astype(int)\n",
    "    class_count = np.bincount(y_tr_np)\n",
    "    class_weight = 1.0 / np.maximum(class_count, 1)\n",
    "    sample_weight = class_weight[y_tr_np]\n",
    "\n",
    "    # Unified CPU RNG (shared by sampler & dataloader)\n",
    "    g_cpu = torch.Generator()\n",
    "    g_cpu.manual_seed(SEED)\n",
    "\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=torch.tensor(sample_weight, dtype=torch.double),\n",
    "        num_samples=len(sample_weight),\n",
    "        replacement=True,\n",
    "        generator=g_cpu\n",
    "    )\n",
    "\n",
    "    batch_size = 128\n",
    "    # For strongest reproducibility: num_workers=0 and pass the same generator\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(X_tr, y_tr),\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        drop_last=False,\n",
    "        num_workers=0,\n",
    "        generator=g_cpu,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    cal_loader = DataLoader(\n",
    "        TensorDataset(X_cal, y_cal),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        generator=g_cpu,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        TensorDataset(X_val, y_val),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        generator=g_cpu,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        TensorDataset(X_te, y_te),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        generator=g_cpu,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "\n",
    "    # Instantiate models after setting randomness (their parameter initialization is controlled by the global seed)\n",
    "    models_config = {\n",
    "        'MLP': MLPModel(input_dim, hidden_dims=[512, 256, 128, 64, 32, 16], dropout=0.3),\n",
    "        'GRU': GRUAttnModel(input_dim, emb_dim=64, hidden_dim=128, num_layers=2, bidirectional=True, dropout=0.3),\n",
    "        'Transformer': TransformerModel(input_dim, d_model=32, nhead=8, num_layers=2, dropout=0.3)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    for name, model in models_config.items():\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Training {name} model\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Train model\n",
    "        trained, best_val_auc, train_losses, val_aucs = train_model(\n",
    "            model, train_loader, val_loader, epochs=120, lr=1e-3, pos_weight=pos_weight\n",
    "        )\n",
    "        \n",
    "        # Evaluate model and perform calibration\n",
    "        eval_results = evaluate_model_with_calibration(trained, cal_loader, test_loader, name)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n{name} Results:\")\n",
    "        print(f\"Best validation AUC: {best_val_auc:.4f}\")\n",
    "        print(f\"Test AUC (original): {eval_results['auc_original']:.4f}\")\n",
    "        print(f\"Test AUC (Platt): {eval_results['auc_platt']:.4f}\")\n",
    "        print(f\"Test AUC (Isotonic): {eval_results['auc_isotonic']:.4f}\")\n",
    "        \n",
    "        print(f\"\\nCalibration metrics:\")\n",
    "        print(f\"ECE (original): {eval_results['calibration_original']['ece']:.4f}\")\n",
    "        print(f\"ECE (Platt): {eval_results['calibration_platt']['ece']:.4f}\")\n",
    "        print(f\"ECE (Isotonic): {eval_results['calibration_isotonic']['ece']:.4f}\")\n",
    "        \n",
    "        print(f\"Brier Score (original): {eval_results['calibration_original']['brier_score']:.4f}\")\n",
    "        print(f\"Brier Score (Platt): {eval_results['calibration_platt']['brier_score']:.4f}\")\n",
    "        print(f\"Brier Score (Isotonic): {eval_results['calibration_isotonic']['brier_score']:.4f}\")\n",
    "\n",
    "        # Classification report\n",
    "        y_hat_bin_original = (eval_results['predictions_original'] > 0.5).astype(int)\n",
    "        y_hat_bin_platt = (eval_results['predictions_platt'] > 0.5).astype(int)\n",
    "        y_hat_bin_isotonic = (eval_results['predictions_isotonic'] > 0.5).astype(int)\n",
    "        \n",
    "        print(f\"\\nClassification report (original):\")\n",
    "        print(classification_report(eval_results['true_labels'], y_hat_bin_original, digits=4))\n",
    "        \n",
    "        # Plot calibration curve\n",
    "        y_prob_dict = {\n",
    "            'Original': eval_results['predictions_original'],\n",
    "            'Platt Scaling': eval_results['predictions_platt'],\n",
    "            'Isotonic Regression': eval_results['predictions_isotonic']\n",
    "        }\n",
    "        plot_calibration_curve(eval_results['true_labels'], y_prob_dict, name)\n",
    "\n",
    "        results[name] = {\n",
    "            'model': trained,\n",
    "            'val_auc': best_val_auc,\n",
    "            'eval_results': eval_results,\n",
    "            'train_losses': train_losses,\n",
    "            'val_aucs': val_aucs\n",
    "        }\n",
    "\n",
    "    # Summary of results\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Final Results Summary\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Model':<12} {'Val AUC':<10} {'Test AUC (Original)':<20} {'Test AUC (Platt)':<18} {'Test AUC (Isotonic)':<21} {'ECE (Original)':<15} {'ECE (Platt)':<13} {'ECE (Isotonic)':<16}\")\n",
    "    print(\"-\" * 120)\n",
    "    \n",
    "    for name in results:\n",
    "        eval_res = results[name]['eval_results']\n",
    "        print(f\"{name:<12} {results[name]['val_auc']:<10.4f} {eval_res['auc_original']:<20.4f} {eval_res['auc_platt']:<18.4f} {eval_res['auc_isotonic']:<21.4f} \"\n",
    "              f\"{eval_res['calibration_original']['ece']:<15.4f} {eval_res['calibration_platt']['ece']:<13.4f} {eval_res['calibration_isotonic']['ece']:<16.4f}\")\n",
    "\n",
    "    # Best model (based on original AUC)\n",
    "    best_model_name = max(results.keys(), key=lambda k: results[k]['eval_results']['auc_original'])\n",
    "    best_auc = results[best_model_name]['eval_results']['auc_original']\n",
    "    print(f\"\\nðŸ† Best model: {best_model_name} (AUC = {best_auc:.4f})\")\n",
    "\n",
    "    # Visualize training process\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Training loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    for name in results:\n",
    "        plt.plot(results[name]['train_losses'], label=name)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Validation AUC\n",
    "    plt.subplot(1, 3, 2)\n",
    "    for name in results:\n",
    "        plt.plot(results[name]['val_aucs'], label=name)\n",
    "    plt.title('Validation AUC')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Test-set AUC comparison (before/after calibration)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    names = list(results.keys())\n",
    "    x = np.arange(len(names))\n",
    "    width = 0.25\n",
    "\n",
    "    original_aucs = [results[n]['eval_results']['auc_original'] for n in names]\n",
    "    platt_aucs = [results[n]['eval_results']['auc_platt'] for n in names]\n",
    "    isotonic_aucs = [results[n]['eval_results']['auc_isotonic'] for n in names]\n",
    "\n",
    "    plt.bar(x - width, original_aucs, width, label='Original', alpha=0.8)\n",
    "    plt.bar(x, platt_aucs, width, label='Platt Scaling', alpha=0.8)\n",
    "    plt.bar(x + width, isotonic_aucs, width, label='Isotonic Regression', alpha=0.8)\n",
    "\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.title('Test-set AUC Comparison')\n",
    "    plt.xticks(x, names)\n",
    "    plt.legend()\n",
    "    plt.ylim(0.7, 1.0)  # Adjust y-axis range to better see differences\n",
    "\n",
    "    # Add value labels\n",
    "    for i, (orig, platt, iso) in enumerate(zip(original_aucs, platt_aucs, isotonic_aucs)):\n",
    "        plt.text(i - width, orig + 0.01, f\"{orig:.3f}\", ha='center', va='bottom', fontsize=8)\n",
    "        plt.text(i, platt + 0.01, f\"{platt:.3f}\", ha='center', va='bottom', fontsize=8)\n",
    "        plt.text(i + width, iso + 0.01, f\"{iso:.3f}\", ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return results, best_model_name\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results, best_model = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf0be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import (brier_score_loss, average_precision_score, \n",
    "                            confusion_matrix, f1_score, precision_recall_curve, \n",
    "                            auc, roc_curve)\n",
    "\n",
    "def calculate_youden_threshold(y_true, y_pred_proba):\n",
    "    \"\"\"\n",
    "    Compute the optimal threshold based on Youden's index.\n",
    "    Youden's index = TPR + TNR - 1 = Sensitivity + Specificity - 1\n",
    "    \"\"\"\n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
    "    \n",
    "    # Compute TNR (specificity) = 1 - FPR\n",
    "    tnr = 1 - fpr\n",
    "    \n",
    "    # Compute Youden's index\n",
    "    youden_scores = tpr + tnr - 1\n",
    "    \n",
    "    # Find threshold corresponding to the maximum Youden's index\n",
    "    optimal_idx = np.argmax(youden_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    optimal_youden = youden_scores[optimal_idx]\n",
    "    \n",
    "    return optimal_threshold, optimal_youden, tpr[optimal_idx], tnr[optimal_idx]\n",
    "\n",
    "# Read ground-truth labels\n",
    "y_true = pd.read_csv(\"mimicdeadtest.csv\")[\"death\"].values\n",
    "\n",
    "# Model files and names\n",
    "model_files = {\n",
    "    \"MLP\": \"death_MLP_calibrated_proba.csv\",\n",
    "    \"GRU\": \"death_GRU_calibrated_proba.csv\", \n",
    "    \"Transformer\": \"death_Transformer_calibrated_proba.csv\"\n",
    "}\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# Subplot 1: Calibration curves\n",
    "plt.subplot(2, 3, 1)\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "# Store evaluation results for all models\n",
    "evaluation_results = {}\n",
    "\n",
    "# For each model, plot calibration curve and compute metrics\n",
    "for i, (model_name, file) in enumerate(model_files.items()):\n",
    "    y_pred = pd.read_csv(file).iloc[:, -2].values  # take the last predicted-probability column\n",
    "    \n",
    "    # Normalize probabilities (divide by 100 if >1)\n",
    "    y_pred = [p * 0.01 if p > 1 else p for p in y_pred]\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Compute calibration curve\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "        y_true, y_pred, n_bins=10, strategy='quantile'\n",
    "    )\n",
    "    \n",
    "    # Get optimal threshold using Youden's index\n",
    "    optimal_threshold, youden_score, optimal_tpr, optimal_tnr = calculate_youden_threshold(y_true, y_pred)\n",
    "    \n",
    "    # Compute metrics based on the optimal threshold\n",
    "    # 1. Brier score\n",
    "    brier_score = brier_score_loss(y_true, y_pred)\n",
    "    \n",
    "    # 2. AUPRC (Area Under Precision-Recall Curve)\n",
    "    auprc = average_precision_score(y_true, y_pred)\n",
    "    \n",
    "    # 3. Use the optimal threshold to compute TPR, TNR, F1\n",
    "    y_pred_binary = (y_pred >= optimal_threshold).astype(int)\n",
    "    \n",
    "    # Compute confusion matrix for verification\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_binary).ravel()\n",
    "    \n",
    "    # Recompute TPR and TNR to ensure consistency\n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    # F1 score\n",
    "    f1 = f1_score(y_true, y_pred_binary)\n",
    "    \n",
    "    # Store results\n",
    "    evaluation_results[model_name] = {\n",
    "        'AUPRC': auprc,\n",
    "        'TPR': tpr,\n",
    "        'TNR': tnr,\n",
    "        'Brier': brier_score,\n",
    "        'F1': f1,\n",
    "        'Threshold': optimal_threshold,\n",
    "        'Youden': youden_score,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    }\n",
    "    \n",
    "    # Plot calibration curve\n",
    "    plt.plot(mean_predicted_value, fraction_of_positives, \"s-\",\n",
    "             label=f'{model_name} (Brier={brier_score:.3f})', \n",
    "             color=colors[i], linewidth=2, markersize=6)\n",
    "\n",
    "# Plot the perfect calibration diagonal\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfect calibration\", linewidth=1)\n",
    "\n",
    "# Set calibration plot properties\n",
    "plt.xlabel(\"Mean predicted probability\", fontsize=12)\n",
    "plt.ylabel(\"Fraction of positives\", fontsize=12)\n",
    "plt.title(\"Calibration curves for hemorrhagic shock prediction models\", fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"upper left\", fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "# Subplot 2: Precision-Recall curve\n",
    "plt.subplot(2, 3, 2)\n",
    "for i, (model_name, file) in enumerate(model_files.items()):\n",
    "    y_pred = pd.read_csv(file).iloc[:, -2].values\n",
    "    y_pred = [p * 0.01 if p > 1 else p for p in y_pred]\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Compute Precision-Recall curve\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "    auprc = evaluation_results[model_name]['AUPRC']\n",
    "    \n",
    "    plt.plot(recall, precision, color=colors[i], linewidth=2,\n",
    "             label=f'{model_name} (AUPRC={auprc:.3f})')\n",
    "\n",
    "plt.xlabel(\"Recall (TPR)\", fontsize=12)\n",
    "plt.ylabel(\"Precision\", fontsize=12)\n",
    "plt.title(\"Precision-Recall Curve\", fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"upper right\", fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "# Subplot 3: ROC curve and Youden's index\n",
    "plt.subplot(2, 3, 3)\n",
    "for i, (model_name, file) in enumerate(model_files.items()):\n",
    "    y_pred = pd.read_csv(file).iloc[:, -1].values\n",
    "    y_pred = [p * 0.01 if p > 1 else p for p in y_pred]\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.plot(fpr, tpr, color=colors[i], linewidth=2,\n",
    "             label=f'{model_name} (AUC={roc_auc:.3f})')\n",
    "    \n",
    "    # Mark the optimal Youden point\n",
    "    optimal_threshold = evaluation_results[model_name]['Threshold']\n",
    "    optimal_tpr = evaluation_results[model_name]['TPR']\n",
    "    optimal_fpr = 1 - evaluation_results[model_name]['TNR']\n",
    "    \n",
    "    plt.plot(optimal_fpr, optimal_tpr, 'o', color=colors[i], markersize=8,\n",
    "             markerfacecolor='white', markeredgewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Random classifier\", linewidth=1)\n",
    "plt.xlabel(\"False Positive Rate (1-TNR)\", fontsize=12)\n",
    "plt.ylabel(\"True Positive Rate (TPR)\", fontsize=12)\n",
    "plt.title(\"ROC Curves (dots mark Youden's optimal points)\", fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "# Subplot 4: Threshold comparison\n",
    "plt.subplot(2, 3, 4)\n",
    "model_names = list(evaluation_results.keys())\n",
    "thresholds = [evaluation_results[model]['Threshold'] for model in model_names]\n",
    "youden_scores = [evaluation_results[model]['Youden'] for model in model_names]\n",
    "\n",
    "bars = plt.bar(model_names, thresholds, color=colors[:len(model_names)], alpha=0.7)\n",
    "plt.xlabel(\"Models\", fontsize=12)\n",
    "plt.ylabel(\"Optimal threshold\", fontsize=12)\n",
    "plt.title(\"Optimal thresholds based on Youden's index\", fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, threshold, youden) in enumerate(zip(bars, thresholds, youden_scores)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{threshold:.3f}\\n(Y={youden:.3f})', \n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Subplot 5: Metric comparison\n",
    "plt.subplot(2, 3, 5)\n",
    "metrics = ['AUPRC', 'TPR', 'TNR', 'F1']\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.2\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    values = [evaluation_results[model][metric] for model in model_names]\n",
    "    plt.bar(x + i*width, values, width, label=metric, alpha=0.8)\n",
    "\n",
    "plt.xlabel(\"Models\", fontsize=12)\n",
    "plt.ylabel(\"Score\", fontsize=12)\n",
    "plt.title(\"Metric comparison\", fontsize=14, fontweight='bold')\n",
    "plt.xticks(x + width*1.5, model_names)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "# Subplot 6: Brier score comparison (lower is better)\n",
    "plt.subplot(2, 3, 6)\n",
    "brier_scores = [evaluation_results[model]['Brier'] for model in model_names]\n",
    "bars = plt.bar(model_names, brier_scores, color=colors[:len(model_names)], alpha=0.7)\n",
    "plt.xlabel(\"Models\", fontsize=12)\n",
    "plt.ylabel(\"Brier score\", fontsize=12)\n",
    "plt.title(\"Brier score comparison (lower is better)\", fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, brier_scores):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "             f'{score:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed evaluation results\n",
    "print(\"=\" * 100)\n",
    "print(\"                          Summary of model performance (optimal thresholds via Youden's index)\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"{'Model':<12} {'AUPRC':<8} {'TPR':<8} {'TNR':<8} {'Brier':<8} {'F1':<8} {'Threshold':<8} {'Youden':<8}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for model_name, results in evaluation_results.items():\n",
    "    print(f\"{model_name:<12} {results['AUPRC']:<8.3f} {results['TPR']:<8.3f} \"\n",
    "          f\"{results['TNR']:<8.3f} {results['Brier']:<8.3f} {results['F1']:<8.3f} \"\n",
    "          f\"{results['Threshold']:<8.3f} {results['Youden']:<8.3f}\")\n",
    "\n",
    "# Detailed confusion matrices\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"                              Detailed confusion matrices\")\n",
    "print(\"=\" * 100)\n",
    "for model_name, results in evaluation_results.items():\n",
    "    print(f\"\\n{model_name} model (threshold={results['Threshold']:.3f}):\")\n",
    "    print(f\"  True positives (TP): {results['TP']:4d}  |  False positives (FP): {results['FP']:4d}\")\n",
    "    print(f\"  False negatives (FN): {results['FN']:4d}  |  True negatives (TN): {results['TN']:4d}\")\n",
    "    print(f\"  Sensitivity (TPR): {results['TPR']:.3f}  |  Specificity (TNR): {results['TNR']:.3f}\")\n",
    "    print(f\"  Youden's index: {results['Youden']:.3f}\")\n",
    "\n",
    "# Metric descriptions\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"                                  Metric descriptions\")\n",
    "print(\"=\" * 100)\n",
    "print(\"â€¢ AUPRC  : Area Under Precision-Recall Curve (higher is better, range 0-1)\")\n",
    "print(\"â€¢ TPR    : True Positive Rate / Sensitivity / Recall (higher is better, range 0-1)\")  \n",
    "print(\"â€¢ TNR    : True Negative Rate / Specificity (higher is better, range 0-1)\")\n",
    "print(\"â€¢ Brier  : Brier Score (lower is better, range 0-1)\")\n",
    "print(\"â€¢ F1     : F1 Score (higher is better, range 0-1)\")\n",
    "print(\"â€¢ Threshold : Optimal probability threshold based on Youden's index\")\n",
    "print(\"â€¢ Youden : Youden's index = TPR + TNR - 1 (higher is better, range 0-1)\")\n",
    "\n",
    "# Additional statistics\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"                               Basic dataset information\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"Total samples: {len(y_true)}\")\n",
    "print(f\"Positives: {np.sum(y_true)} ({np.mean(y_true):.1%})\")\n",
    "print(f\"Negatives: {len(y_true) - np.sum(y_true)} ({1-np.mean(y_true):.1%})\")\n",
    "\n",
    "# Find the best models\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"                                Best model picks\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "best_models = {\n",
    "    'AUPRC': max(evaluation_results.items(), key=lambda x: x[1]['AUPRC']),\n",
    "    'F1': max(evaluation_results.items(), key=lambda x: x[1]['F1']),\n",
    "    'Brier': min(evaluation_results.items(), key=lambda x: x[1]['Brier']),\n",
    "    'TPR': max(evaluation_results.items(), key=lambda x: x[1]['TPR']),\n",
    "    'TNR': max(evaluation_results.items(), key=lambda x: x[1]['TNR']),\n",
    "    'Youden': max(evaluation_results.items(), key=lambda x: x[1]['Youden'])\n",
    "}\n",
    "\n",
    "for metric, (model_name, results) in best_models.items():\n",
    "    print(f\"â€¢ Best {metric} model: {model_name} ({metric}={results[metric]:.3f})\")\n",
    "\n",
    "# Advantages of Youden's index method\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"                             Advantages of the Youden's index method\")\n",
    "print(\"=\" * 100)\n",
    "print(\"âœ“ Automatically finds the threshold that balances sensitivity and specificity for each model\")\n",
    "print(\"âœ“ Avoids issues with a fixed threshold (e.g., 0.5) that may not suit the actual data distribution\")  \n",
    "print(\"âœ“ Youden's index = TPR + TNR - 1; maximizing it jointly optimizes sensitivity and specificity\")\n",
    "print(\"âœ“ Especially suitable for medical diagnostics where both false positives and false negatives matter\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
